{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca888824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/amiteshgangrade/anaconda3/lib/python3.10/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check\\\n",
    "  torch==1.13.1\\\n",
    "  torchdata==0.5.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184638e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "  transformers==4.27.2 \\\n",
    "  datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51733751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ded313",
   "metadata": {},
   "source": [
    "## Summarize Dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3a2bf",
   "metadata": {},
   "source": [
    "##### Use Case : Generation summary of a dialogue with pre-train LLM FLAN-T5 from HuggingFace. \n",
    "\n",
    "##### Loading dataset from Hugging Face DialogSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b223404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 4.56k/4.56k [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to /Users/amiteshgangrade/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/11.3M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  11%|██▎                 | 1.28M/11.3M [00:00<00:01, 8.64MB/s]\u001b[A\n",
      "Downloading data:  19%|███▉                | 2.20M/11.3M [00:00<00:01, 8.88MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▍              | 3.09M/11.3M [00:00<00:01, 6.72MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▍            | 4.23M/11.3M [00:00<00:00, 7.91MB/s]\u001b[A\n",
      "Downloading data:  45%|████████▉           | 5.07M/11.3M [00:00<00:00, 7.50MB/s]\u001b[A\n",
      "Downloading data:  52%|██████████▎         | 5.85M/11.3M [00:00<00:00, 7.17MB/s]\u001b[A\n",
      "Downloading data:  58%|███████████▋        | 6.59M/11.3M [00:00<00:00, 7.01MB/s]\u001b[A\n",
      "Downloading data:  64%|████████████▉       | 7.30M/11.3M [00:01<00:00, 6.37MB/s]\u001b[A\n",
      "Downloading data:  71%|██████████████▏     | 8.07M/11.3M [00:01<00:00, 6.68MB/s]\u001b[A\n",
      "Downloading data:  77%|███████████████▍    | 8.75M/11.3M [00:01<00:00, 6.55MB/s]\u001b[A\n",
      "Downloading data:  84%|████████████████▋   | 9.46M/11.3M [00:01<00:00, 6.67MB/s]\u001b[A\n",
      "Downloading data:  90%|█████████████████▉  | 10.1M/11.3M [00:01<00:00, 6.46MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 11.3M/11.3M [00:01<00:00, 6.96MB/s]\u001b[A\n",
      "Downloading data files:  33%|███████              | 1/3 [00:02<00:05,  2.84s/it]\n",
      "Downloading data:   0%|                             | 0.00/1.35M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▌                    | 33.8k/1.35M [00:00<00:06, 193kB/s]\u001b[A\n",
      "Downloading data:   7%|█▌                   | 99.3k/1.35M [00:00<00:04, 289kB/s]\u001b[A\n",
      "Downloading data:  18%|████                  | 248k/1.35M [00:00<00:02, 525kB/s]\u001b[A\n",
      "Downloading data:  39%|████████▌             | 525k/1.35M [00:00<00:00, 911kB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 1.35M/1.35M [00:00<00:00, 1.45MB/s]\u001b[A\n",
      "Downloading data files:  67%|██████████████       | 2/3 [00:05<00:02,  2.52s/it]\n",
      "Downloading data:   0%|                              | 0.00/442k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   6%|█▎                    | 25.6k/442k [00:00<00:03, 136kB/s]\u001b[A\n",
      "Downloading data:  22%|████▉                 | 99.3k/442k [00:00<00:01, 293kB/s]\u001b[A\n",
      "Downloading data: 100%|███████████████████████| 442k/442k [00:00<00:00, 777kB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 3/3 [00:07<00:00,  2.34s/it]\n",
      "Extracting data files: 100%|█████████████████████| 3/3 [00:00<00:00, 804.95it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/amiteshgangrade/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1222.95it/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7398e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example  1\n",
      "___________________________________________________________________________________________________\n",
      "Input Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "___________________________________________________________________________________________________\n",
      "BaseLine Human Summary : \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "___________________________________________________________________________________________________\n",
      "\n",
      "___________________________________________________________________________________________________\n",
      "Example  2\n",
      "___________________________________________________________________________________________________\n",
      "Input Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "___________________________________________________________________________________________________\n",
      "BaseLine Human Summary : \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "___________________________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40,200]\n",
    "\n",
    "dash_line = '_'.join('' for x in range(100))\n",
    "for i , index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print(\"Input Dialogue:\")\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BaseLine Human Summary : ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8eae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading pytorch_model.bin:   0%|                 | 0.00/990M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 10.5M/990M [00:01<02:17, 7.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 21.0M/990M [00:03<02:24, 6.72MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 31.5M/990M [00:04<02:26, 6.57MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 41.9M/990M [00:06<02:24, 6.55MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 52.4M/990M [00:07<02:23, 6.53MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 62.9M/990M [00:09<02:22, 6.50MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 73.4M/990M [00:11<02:21, 6.50MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▋       | 83.9M/990M [00:12<02:22, 6.37MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 94.4M/990M [00:14<02:20, 6.39MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▉        | 105M/990M [00:16<02:24, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|█        | 115M/990M [00:18<02:28, 5.88MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█▏       | 126M/990M [00:20<02:25, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█▏       | 136M/990M [00:21<02:24, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▎       | 147M/990M [00:23<02:22, 5.91MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▍       | 157M/990M [00:25<02:27, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▌       | 168M/990M [00:27<02:22, 5.75MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▌       | 178M/990M [00:29<02:16, 5.94MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▋       | 189M/990M [00:30<02:13, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▊       | 199M/990M [00:32<02:08, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▉       | 210M/990M [00:33<02:05, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|██       | 220M/990M [00:35<02:06, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|██       | 231M/990M [00:37<02:02, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|██▏      | 241M/990M [00:39<02:00, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|██▎      | 252M/990M [00:40<01:59, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|██▍      | 262M/990M [00:42<01:55, 6.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|██▍      | 273M/990M [00:44<01:53, 6.34MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██▌      | 283M/990M [00:45<01:53, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██▋      | 294M/990M [00:47<01:50, 6.32MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▊      | 304M/990M [00:48<01:47, 6.38MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▊      | 315M/990M [00:50<01:49, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▉      | 325M/990M [00:52<01:45, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|███      | 336M/990M [00:54<01:43, 6.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|███▏     | 346M/990M [00:55<01:44, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  36%|███▏     | 357M/990M [00:57<01:41, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|███▎     | 367M/990M [00:59<01:38, 6.34MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|███▍     | 377M/990M [01:00<01:37, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|███▌     | 388M/990M [01:02<01:35, 6.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|███▌     | 398M/990M [01:04<01:33, 6.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|███▋     | 409M/990M [01:05<01:32, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|███▊     | 419M/990M [01:07<01:30, 6.32MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|███▉     | 430M/990M [01:09<01:28, 6.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|████     | 440M/990M [01:10<01:27, 6.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|████     | 451M/990M [01:12<01:24, 6.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|████▏    | 461M/990M [01:14<01:25, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|████▎    | 472M/990M [01:15<01:22, 6.32MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|████▍    | 482M/990M [01:17<01:19, 6.36MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|████▍    | 493M/990M [01:18<01:18, 6.36MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|████▌    | 503M/990M [01:20<01:18, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|████▋    | 514M/990M [01:22<01:15, 6.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|████▊    | 524M/990M [01:23<01:13, 6.36MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  54%|████▊    | 535M/990M [01:25<01:13, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|████▉    | 545M/990M [01:27<01:10, 6.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|█████    | 556M/990M [01:28<01:08, 6.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|█████▏   | 566M/990M [01:30<01:07, 6.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|█████▏   | 577M/990M [01:32<01:05, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|█████▎   | 587M/990M [01:33<01:02, 6.40MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|█████▍   | 598M/990M [01:35<01:02, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|█████▌   | 608M/990M [01:37<01:00, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|█████▌   | 619M/990M [01:39<01:03, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|█████▋   | 629M/990M [01:40<00:58, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|█████▊   | 640M/990M [01:42<00:57, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|█████▉   | 650M/990M [01:44<00:54, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|██████   | 661M/990M [01:45<00:52, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|██████   | 671M/990M [01:47<00:51, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|██████▏  | 682M/990M [01:49<00:48, 6.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|██████▎  | 692M/990M [01:50<00:47, 6.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|██████▍  | 703M/990M [01:52<00:46, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|██████▍  | 713M/990M [01:54<00:43, 6.32MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|██████▌  | 724M/990M [01:55<00:42, 6.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|██████▋  | 734M/990M [01:57<00:40, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|██████▊  | 744M/990M [01:59<00:38, 6.36MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|██████▊  | 755M/990M [02:01<00:39, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|██████▉  | 765M/990M [02:03<00:38, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|███████  | 776M/990M [02:04<00:34, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|███████▏ | 786M/990M [02:06<00:32, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|███████▏ | 797M/990M [02:07<00:31, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|███████▎ | 807M/990M [02:09<00:29, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|███████▍ | 818M/990M [02:11<00:27, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|███████▌ | 828M/990M [02:12<00:25, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|███████▌ | 839M/990M [02:14<00:24, 6.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|███████▋ | 849M/990M [02:16<00:22, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|███████▊ | 860M/990M [02:17<00:20, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|███████▉ | 870M/990M [02:19<00:19, 6.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|████████ | 881M/990M [02:21<00:17, 6.36MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|████████ | 891M/990M [02:22<00:15, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|████████▏| 902M/990M [02:24<00:13, 6.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|████████▎| 912M/990M [02:26<00:12, 6.38MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|████████▍| 923M/990M [02:27<00:10, 6.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|████████▍| 933M/990M [02:29<00:09, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|████████▌| 944M/990M [02:31<00:07, 6.32MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|████████▋| 954M/990M [02:32<00:05, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|████████▊| 965M/990M [02:34<00:04, 6.34MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|████████▊| 975M/990M [02:36<00:02, 6.37MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|████████▉| 986M/990M [02:37<00:00, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|█████████| 990M/990M [02:38<00:00, 6.25MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)neration_config.json: 100%|██████| 147/147 [00:00<00:00, 312kB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cbcb631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)okenizer_config.json: 100%|█| 2.54k/2.54k [00:00<00:00, 4.31MB/s]\u001b[A\n",
      "\n",
      "Downloading spiece.model: 100%|██████████████| 792k/792k [00:00<00:00, 14.4MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 2.42M/2.42M [00:01<00:00, 2.15MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)cial_tokens_map.json: 100%|█| 2.20k/2.20k [00:00<00:00, 5.19MB/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c37a8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 363,   97,   19,   34,    3,    6, 3059,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Encoded Sentence :\n",
      "{'input_ids': tensor([[ 363,   97,   19,   34,    3,    6, 3059,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Decoded Sentence : \n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it , Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence,return_tensors='pt')\n",
    "print(sentence_encoded)\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "    sentence_encoded[\"input_ids\"][0],\n",
    "    skip_special_tokens = True\n",
    ")\n",
    "print(\"Encoded Sentence :\")\n",
    "print(sentence_encoded)\n",
    "print('Decoded Sentence : ')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b2c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104942f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00f155c5",
   "metadata": {},
   "source": [
    "### Summarize Dialogue with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ae86e",
   "metadata": {},
   "source": [
    "###### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ce5a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example  1\n",
      "___________________________________________________________________________________________________\n",
      "INPUT Propmt:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary :\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "___________________________________________________________________________________________________\n",
      "Model Generation - Without Prompt Engineering : \n",
      "Person1: It's ten to nine.\n",
      "___________________________________________________________________________________________________\n",
      "Example  2\n",
      "___________________________________________________________________________________________________\n",
      "INPUT Propmt:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary :\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "___________________________________________________________________________________________________\n",
      "Model Generation - Without Prompt Engineering : \n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "    {dialogue}\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(dialogue,return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=50,\n",
    "        )[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print(\"Example \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT Propmt:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'Baseline Human Summary :\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'Model Generation - Without Prompt Engineering : \\n{output}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f9db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5307947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Example :  1\n",
      "___________________________________________________________________________________________________\n",
      "Input Prompt : \n",
      "\n",
      "    Dialogue :\n",
      "    #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "    What was going on?\n",
      "    \n",
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary : \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "___________________________________________________________________________________________________\n",
      "Model Generation - Zero Shot : \n",
      "Tom\n",
      "___________________________________________________________________________________________________\n",
      "Example :  2\n",
      "___________________________________________________________________________________________________\n",
      "Input Prompt : \n",
      "\n",
      "    Dialogue :\n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on?\n",
      "    \n",
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary : \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "___________________________________________________________________________________________________\n",
      "Model Generation - Zero Shot : \n",
      "#\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Dialogue :\n",
    "    {dialogue}\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=True\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(dash_line)\n",
    "    print(\"Example : \", i+1)\n",
    "    print(dash_line)\n",
    "    print(f'Input Prompt : \\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'Baseline Human Summary : \\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f\"Model Generation - Zero Shot : \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b6c456",
   "metadata": {},
   "source": [
    "### Summarize Dialouge with One Shot and Few Shot Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa2824",
   "metadata": {},
   "source": [
    "One Shot and Few Shot Inferences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c8b09",
   "metadata": {},
   "source": [
    "### One Shot Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e597f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d78bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    \n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "        Dialogue : \n",
    "        {dialogue}\n",
    "        What was going on?\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "        \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    prompt += f\"\"\"\n",
    "    Dialogue : \n",
    "    {dialogue}\n",
    "    What was going on ?\n",
    "    \"\"\"\n",
    "        \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eeb8577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue : \n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "    Dialogue : \n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on ?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c230e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary : \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "___________________________________________________________________________________________________\n",
      "Model Generation - One Shot Infenrence : \n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt,return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary : \\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - One Shot Infenrence : \\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699fcc6c",
   "metadata": {},
   "source": [
    "### Few Shot Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f118bcc",
   "metadata": {},
   "source": [
    "Few Shot Inference by adding two more full dialogue-summay pairs to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25d1e99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Dialogue : \n",
      "        #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "        What was going on?\n",
      "        #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "        \n",
      "        Dialogue : \n",
      "        #Person1#: Hey, Frank. I heard you got a new job.\n",
      "#Person2#: Yeah, Judy. I will be working for the Post Office. It's not a bad job.\n",
      "#Person1#: Is it true that you have a heavy work schedule?\n",
      "#Person2#: That's right. I am supposed to work at 5am everyday, and I only get 45 minutes for lunch.\n",
      "#Person1#: So, why did you apply for such a demanding job?\n",
      "#Person2#: Well, the government offers its employees excellent health insurance benefits.\n",
      "#Person1#: Oh, I see. And can your family members utilize the health insurance, too?\n",
      "#Person2#: Yeah, that's the best part. All of my children can get free medical care.\n",
      "#Person1#: That's a great employment benefit!\n",
      "#Person2#: Now you know why I wanted to work for the Post Office!\n",
      "        What was going on?\n",
      "        Frank got a new job and is telling Judy not only the heavy schedule but also the benefits of this job.\n",
      "        \n",
      "        Dialogue : \n",
      "        #Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "        What was going on?\n",
      "        #Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "        \n",
      "    Dialogue : \n",
      "    #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "    What was going on ?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40,60,120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_propmt = make_prompt(example_indices_full,example_index_to_summarize)\n",
    "\n",
    "print(few_shot_propmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df2e140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "___________________________________________________________________________________________________\n",
      "Human Generation - Few Shot Inference : #Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "input = tokenizer(few_shot_propmt,return_tensors='pt')\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        input['input_ids'],\n",
    "        max_new_tokens=50,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'Human Generation - Few Shot Inference : {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358aa68",
   "metadata": {},
   "source": [
    "Model Does not Perform that good with Few Shot learning as well , so it is better to fine tune the model or to use another model to work on the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d4c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
